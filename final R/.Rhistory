#HIT CHANGE COLOR
tkplot(g)
#plot graph
#NOTICE this loads in a differend window unlike the wordcloud which loads in the plots tab
#TO CHANGE THE VERTICES COLOR, CLICK THE SELECT BUTTON IN THE TOP LEFT CORNER
#OF THE GRAPHS WINDOW AND HIT 'SELECT ALL VERTICES', THEN RIGHT CLICK A VERTEX AND
#HIT CHANGE COLOR
tkplot(g)
#This searches twitter for the string containing 'programming' and returns
#the 500 most recent tweets
tweets <- searchTwitter('KGF', lang = "en", n = 500, resultType="recent")
(n.tweet<-length(tweets))#gives the number of tweets
#converting the tweets file in to a dataframe
tweets.df <- twListToDF(tweets)
tweets.df
summary(tweets.df)#summary of the data
#details with respect to each variable in the twitter dataset
names(tweets.df)
dim(tweets.df)#Gives the dimentions of the matrix holding the tweets
str(tweets.df)
attributes(tweets.df)
df <- do.call("rbind", lapply(tweets, as.data.frame))#take tweets and call into another dataframe
#names(df)
#head(df,3)
counts=table(df$text)
#we then transform these tweets into a vector to begin using the data
tweets_text <- sapply(tweets, function(x) x$getText())
#create a corpus of the tweets which is from the tm library so we can preform
#edit operations on the text in the tweets next
tweet_corpus <- Corpus(VectorSource(tweets_text))
#First we remove the punctuation because we dont need it
#make all of the words lowercase to make it easier
#stop unwanted 'english' words ex(and, is, then, if, etc.)
#remove numbers
#remove the now frequent whitespace because a lot of stuff was removed
#remove words that are the same as search criteria
#remove url
tweet_corpus <- tm_map(tweet_corpus, removePunctuation)
tweet_corpus#show the contents of the corpus
tweet_corpus <- tm_map(tweet_corpus, content_transformer(tolower))
tweet_corpus <- tm_map(tweet_corpus, removeWords, stopwords("english"))
tweet_corpus <- tm_map(tweet_corpus, removeNumbers)
tweet_corpus <- tm_map(tweet_corpus, stripWhitespace)
tweet_corpus <- tm_map(tweet_corpus, removeWords, c("programming","program"))
tweet_corpus<-tm_map(tweet_corpus,removeURL)
removeURL<-function(x) gsub("http[[:alnum:]]*","",x)
tweet_stopwords<-c(stopwords("english"),"available","via","and")
tweet_stopwords<-setdiff(tweet_stopwords,c("r","big"))
tweet_corpus<-tm_map(tweet_corpus,removeWords,tweet_stopwords)
tweet_corpuscopy<-tweet_corpus
#turn the vector corpus of tweets into and editable text document containing the matrix
tdm<-TermDocumentMatrix(tweet_corpus,control = list(wordlenths=c(1,Inf)))
tdm
idm<-which(dimnames(tdm)$terms == "r")
(frequency.terms<-findFreqTerms(tdm,lowfreq = 10))#show terms with a freq higher than 10
term.freq<-rowSums(as.matrix(tdm))
term.freq<-subset(term.freq,term.freq>=10)
df<-data.frame(term =names(term.freq),freq=term.freq)
#sort the words in decending order
#show rough view and count
v <- sort(term.freq,decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
d
#histogram for the words which are most frequently used in the tweets
#criteria we used here is#if the word appears atleast 10 times in the tweets data
#the word is characterised as a frequent word.
ggplot(df, aes(x=term,y=freq))+geom_bar(stat = "identity")+xlab("terms")+ylab("count")+coord_flip()
#create list of words with their relation to 'javascript' as a ratio
word.co <- findAssocs(tdm, "javascript", corlimit=0.0)
head(word.co, 20)
topWords <- word.co$javascript[1:10]
barplot(topWords, main="Top Tweets About Programming",
xlab="The Words")
#this function now creates the word cloud from the new stripped down tweets above
#we pass in the tweet vector
#make the words in a random order
#use only a max of 100 words (dont wwant it too big)
#scale the size of the wordcloud
#add colors (this color changes as you get further from the center which means less use of those words)
wordcloud(tweet_corpus, random.order = F, max.words = 100, scale = c(3,0.5), colors = brewer.pal(8,'Dark2'))
#now this is the beginning of taking a user from twitter and creating a bubble graph
#of their followers and who they follow
#get our twitter user
start <- getUser("@ashwin_rishi")
topWords <- word.co$javascript[1:10]
barplot(topWords, main="Top Tweets About Programming",
xlab="The Words")
#sort the words in decending order
#show rough view and count
v <- sort(term.freq,decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
d
#histogram for the words which are most frequently used in the tweets
#criteria we used here is#if the word appears atleast 10 times in the tweets data
#the word is characterised as a frequent word.
ggplot(df, aes(x=term,y=freq))+geom_bar(stat = "identity")+xlab("terms")+ylab("count")+coord_flip()
#create list of words with their relation to 'javascript' as a ratio
word.co <- findAssocs(tdm, "javascript", corlimit=0.0)
head(word.co, 20)
topWords <- word.co$javascript[1:10]
barplot(topWords, main="Top Tweets About Programming",
xlab="The Words")
#install.packages("plotrix")
library(plotrix)
#pie(slices, labels = labels, col=rainbow(length(labels)), main="Sentiment Analysis")
pie3D(slices, labels = labels, col=rainbow(length(labels)),explode=0.00, main="Twitter Sentiment Analysis of Samsung S9--(Release date Feb 25,2018)")
consumer_key <-"jME9uoqLYAJ3CYDkAWOTmADj1"
consumer_secret <- "toWLd9BEm6KIjmYgLWzgB4U4HdQLTPVI2KjJmk5CMB7Wu1AVu1"
access_token<-"1719665600-EEvvPyi08FqacoOT5c9rRSOp223xx7OsEhMse05"
access_secret <- "KHnjkS9UQKphGyXimRl4LGTeOdgXP0nOIhZ7cMi8HttJ2"
#install.packages("httr")
library(httr)
# 1. Find OAuth settings for twitter:
#    https://dev.twitter.com/docs/auth/oauth
oauth_endpoints("twitter")
# 2. Register an application at https://apps.twitter.com/
#    Make sure to set callback url to "http://127.0.0.1:1410/"
#
#    Replace key and secret below
myapp <- oauth_app("twitter",
key = consumer_key,
secret = consumer_secret
)
# 3. Get OAuth credentials
twitter_token <- oauth1.0_token(oauth_endpoints("twitter"), myapp)
# 4. Use API
req <- GET("https://api.twitter.com/1.1/statuses/home_timeline.json",config(token = twitter_token))
consumer_key <-"WfXMTCCC0C7h1GWQvAuUpiD6v"
consumer_secret <- "P0jMsqwfwekow6bPUc4BLxeCLfMKIkIsCqaanfgrnoMFy6IJI1"
access_token<-"523870397-EcuX2XaykeHfTwAunmHSc5Zm10ZxBglaq0Tljt85"
access_secret <- "5NrT9ICifGtainJgXx7QR00MSdPjY759ZYhMijb66IHLK"
#install.packages("httr")
library(httr)
# 1. Find OAuth settings for twitter:
#    https://dev.twitter.com/docs/auth/oauth
oauth_endpoints("twitter")
# 2. Register an application at https://apps.twitter.com/
#    Make sure to set callback url to "http://127.0.0.1:1410/"
#
#    Replace key and secret below
myapp <- oauth_app("twitter",
key = consumer_key,
secret = consumer_secret
)
# 3. Get OAuth credentials
twitter_token <- oauth1.0_token(oauth_endpoints("twitter"), myapp)
# 4. Use API
req <- GET("https://api.twitter.com/1.1/statuses/home_timeline.json",config(token = twitter_token))
stop_for_status(req)
content(req)
#install.packages("twitteR")
#and
#install.packages("ROAuth")
library(ROAuth)
library(twitteR)
#install.packages("devtools")
library(devtools)
download.file(url="http://curl.haxx.se/ca/cacert.pem", destfile="cacert.pem") #downloads the certificate
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
cred <- OAuthFactory$new(consumerKey=consumer_key,
consumerSecret=consumer_secret,
requestURL='https://api.twitter.com/oauth/request_token',
accessURL='https://api.twitter.com/oauth/access_token',
authURL='https://api.twitter.com/oauth/authorize')
cred$handshake(cainfo="cacert.pem")
#Extract tweets. Example-
s9.tweets = searchTwitter("sarkar",lang = "en", n=25000)
library(plyr)
library(dplyr)
library(twitteR)
library(ROAuth)
library(wordcloud)
library(tm)
library(igraph)
library(ggplot2)
consumer_key <- "WfXMTCCC0C7h1GWQvAuUpiD6v"
consumer_secret <- "P0jMsqwfwekow6bPUc4BLxeCLfMKIkIsCqaanfgrnoMFy6IJI1"
access_token <- "523870397-EcuX2XaykeHfTwAunmHSc5Zm10ZxBglaq0Tljt85"
access_secret <- "5NrT9ICifGtainJgXx7QR00MSdPjY759ZYhMijb66IHLK"
print(msg);
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
#This searches twitter for the string containing 'programming' and returns
#the 500 most recent tweets
tweets <- searchTwitter("Sarkar", lang = "en", n = 20, resultType="recent")
(n.tweet<-length(tweets))#gives the number of tweets
#converting the tweets file in to a dataframe
tweets.df <- twListToDF(tweets)
tweets.df
summary(tweets.df)#summary of the data
#details with respect to each variable in the twitter dataset
names(tweets.df)
dim(tweets.df)#Gives the dimentions of the matrix holding the tweets
str(tweets.df)
attributes(tweets.df)
df <- do.call("rbind", lapply(tweets, as.data.frame))#take tweets and call into another dataframe
#names(df)
#head(df,3)
counts=table(df$text)
#we then transform these tweets into a vector to begin using the data
tweets_text <- sapply(tweets, function(x) x$getText())
tweets_text
#create a corpus of the tweets which is from the tm library so we can preform
#edit operations on the text in the tweets next
tweet_corpus <- Corpus(VectorSource(tweets_text))
#First we remove the punctuation because we dont need it
#make all of the words lowercase to make it easier
#stop unwanted 'english' words ex(and, is, then, if, etc.)
#remove numbers
#remove the now frequent whitespace because a lot of stuff was removed
#remove words that are the same as search criteria
#remove url
tweet_corpus <- tm_map(tweet_corpus, removePunctuation)
tweet_corpus#show the contents of the corpus
tweet_corpus <- tm_map(tweet_corpus, content_transformer(tolower))
tweet_corpus <- tm_map(tweet_corpus, removeWords, stopwords("english"))
tweet_corpus <- tm_map(tweet_corpus, removeNumbers)
tweet_corpus <- tm_map(tweet_corpus, stripWhitespace)
tweet_corpus <- tm_map(tweet_corpus, removeWords, c("programming","program"))
removeURL<-function(x) gsub("http[[:alnum:]]*","",x)
tweet_corpus<-tm_map(tweet_corpus,removeURL)
tweet_stopwords<-c(stopwords("english"),"available","via","and")
tweet_stopwords<-setdiff(tweet_stopwords,c("r","big"))
tweet_corpus<-tm_map(tweet_corpus,removeWords,tweet_stopwords)
tweet_corpuscopy<-tweet_corpus
#turn the vector corpus of tweets into and editable text document containing the matrix
tdm<-TermDocumentMatrix(tweet_corpus,control = list(wordlenths=c(1,Inf)))
View(dimnames(tdm))
idm<-which(dimnames(tdm)$terms == "r")
(frequency.terms<-findFreqTerms(tdm,lowfreq = 35))#show terms with a freq higher than 10
term.freq<-rowSums(as.matrix(tdm))
term.freq<-subset(term.freq,term.freq>=20)
df<-data.frame(term =names(term.freq),freq=term.freq)
View(df)
library (tuber)
app_id <- "1088891801215-srtv9mvhb4dt1qr62254vfcgqfg9cp33.apps.googleusercontent.com"
app_secret <- "hyJRTIMMvnBNj6WKE6jIzcXa"
yt_oauth(app_id, app_secret)
install.packages("tuber")
library (tuber)
app_id <- "1088891801215-srtv9mvhb4dt1qr62254vfcgqfg9cp33.apps.googleusercontent.com"
app_secret <- "hyJRTIMMvnBNj6WKE6jIzcXa"
yt_oauth(app_id, app_secret)
library (tuber)
app_id <- "1088891801215-srtv9mvhb4dt1qr62254vfcgqfg9cp33.apps.googleusercontent.com"
app_secret <- "hyJRTIMMvnBNj6WKE6jIzcXa"
yt_oauth(app_id, app_secret)
library (tuber)
app_id <- "151536471484-alilbo884ksok19o0bh3vltkdjbcnp48.apps.googleusercontent.com"
app_secret <-"LR4cfWz9kkJ1N_09LbxMYtAj"
yt_oauth(app_id, app_secret)
yt_oauth(app_id, app_secret,token='')
yt_search("Barack Obama")
library (tuber)
app_id <- "151536471484-alilbo884ksok19o0bh3vltkdjbcnp48.apps.googleusercontent.com"
app_secret <-"LR4cfWz9kkJ1N_09LbxMYtAj"
yt_oauth(app_id, app_secret,token='')
yt_search("Barack Obama")
yt_search(term="Barack Obama", max_results=5,channel_id = NULL)
yt_search(term="Barack Obama", max_results=5)
library (tuber)
app_id <- "151536471484-alilbo884ksok19o0bh3vltkdjbcnp48.apps.googleusercontent.com"
app_secret <-"LR4cfWz9kkJ1N_09LbxMYtAj"
yt_oauth(app_id, app_secret,token='')
yt_search(term="Barack Obama", max_results=5)
yt_search("Barack Obama", max_results=5)
yt_search(term = "Barack Obama", max_results=5)
yt_search("Barack Obama")
library (tuber)
app_id <- "151536471484-alilbo884ksok19o0bh3vltkdjbcnp48.apps.googleusercontent.com"
app_secret <-"LR4cfWz9kkJ1N_09LbxMYtAj"
yt_oauth(app_id, app_secret,token='')
yt_search("Barack Obama")
library (tuber)
app_id <- "151536471484-alilbo884ksok19o0bh3vltkdjbcnp48.apps.googleusercontent.com"
app_secret <-"LR4cfWz9kkJ1N_09LbxMYtAj"
yt_oauth(app_id, app_secret,token='')
res<-yt_search("2.0")
head(res[,1:3])
app_id <- "151536471484-alilbo884ksok19o0bh3vltkdjbcnp48.apps.googleusercontent.com"
app_secret <-"LR4cfWz9kkJ1N_09LbxMYtAj"
yt_oauth(app_id, app_secret,token='')
res<-yt_search("2.0")
a <- yt_search(term = "Barack Obama", max_results = 10, get_all = FALSE)
nrow(a)
yt_oauth(app_id, app_secret)
library (tuber)
app_id <- "151536471484-alilbo884ksok19o0bh3vltkdjbcnp48.apps.googleusercontent.com"
app_secret <-"LR4cfWz9kkJ1N_09LbxMYtAj"
yt_oauth(app_id, app_secret)
library (tuber)
app_id <- "151536471484-alilbo884ksok19o0bh3vltkdjbcnp48.apps.googleusercontent.com"
app_secret <-"LR4cfWz9kkJ1N_09LbxMYtAj"
yt_oauth(app_id, app_secret)
library (tuber)
app_id <- "151536471484-31i2ef8p58b491lcgvatvj6tpshh8hkq.apps.googleusercontent.com"
app_secret <-"Ik39OEUr3bNfOvvPjfIof_km"
yt_oauth(app_id, app_secret)
library (tuber)
app_id <- "151536471484-m7i7sch428q0r9ut23m1b1g2crusvip0.apps.googleusercontent.com"
app_secret <-"3HoQ56HxGVL7DMftgq9JxsOl"
yt_oauth(app_id, app_secret, token = "")
library (tuber)
app_id <- "151536471484-m7i7sch428q0r9ut23m1b1g2crusvip0.apps.googleusercontent.com"
app_secret <-"3HoQ56HxGVL7DMftgq9JxsOl"
yt_oauth(app_id, app_secret, token = "")
library (tuber)
app_id <- "151536471484-cr9ra69jv4k37cj64ccls0cta3mgnmog.apps.googleusercontent.com"
app_secret <-"3u9G6ztIFj3GLlAcLxSDthpC"
yt_oauth(app_id, app_secret, token = "")
library (tuber)
app_id <- "151536471484-cr9ra69jv4k37cj64ccls0cta3mgnmog.apps.googleusercontent.com"
app_secret <-"3u9G6ztIFj3GLlAcLxSDthpC"
yt_oauth(app_id, app_secret, token = "")
library(plumber)
getwd()
r <- plumb("C:/Users/AR064679/Documents/thesis/final R/youtube.R")
library(plumber)
getwd()
r <- plumb("C:/Users/AR064679/Documents/thesis/final R/youtube.R")
library(plumber)
getwd()
r <- plumb("C:/Users/AR064679/Documents/thesis/final R/youtube.R")
library(plumber)
getwd()
r <- plumb("C:/Users/AR064679/Documents/thesis/final R/youtube.R")
library(plumber)
getwd()
r <- plumb("C:/Users/AR064679/Documents/thesis/final R/youtube.R")
library (tuber)
app_id <- "151536471484-cr9ra69jv4k37cj64ccls0cta3mgnmog.apps.googleusercontent.com"
app_secret <-"3u9G6ztIFj3GLlAcLxSDthpC"
yt_oauth(app_id, app_secret, token = "")
library (tuber)
app_id <- "151536471484-cr9ra69jv4k37cj64ccls0cta3mgnmog.apps.googleusercontent.com"
app_secret <-"3u9G6ztIFj3GLlAcLxSDthpC"
yt_oauth(app_id, app_secret, token = "")
library (tuber)
app_id <- "151536471484-cr9ra69jv4k37cj64ccls0cta3mgnmog.apps.googleusercontent.com"
app_secret <-"3u9G6ztIFj3GLlAcLxSDthpC"
yt_oauth(app_id, app_secret, token = "")
library (tuber)
app_id <- "151536471484-7s1bbq96drpdj03l00s0turepor9fpah.apps.googleusercontent.com"
app_secret <-"HygKXzL_WrnyLwiTfFUzsGMX"
yt_oauth(app_id, app_secret, token = "")
library (tuber)
app_id <- "151536471484-7s1bbq96drpdj03l00s0turepor9fpah.apps.googleusercontent.com"
app_secret <-"HygKXzL_WrnyLwiTfFUzsGMX"
yt_oauth(app_id, app_secret)
res<-yt_search(term = "KGF movie reviews")
head(res[,1:3])
library (tuber)
app_id <- "151536471484-7s1bbq96drpdj03l00s0turepor9fpah.apps.googleusercontent.com"
app_secret <-"HygKXzL_WrnyLwiTfFUzsGMX"
yt_oauth(app_id, app_secret, token = "")
res<-yt_search(term = "KGF movie reviews")
head(res[,1:3])
head(res[,5:3])
res<-yt_search(term = "KGF movie reviews")
head(res[,5:3])
head(res[,2:3])
head(res[,1:7])
head(res[2,1:7])
View(res)
res<-yt_search(term = "KGF movie reviews")
View(res)
res<-yt_search(term = "winston speech")
View(res)
head(res[,1:7])
head(res[video_id,1:5])
head(res[,1:5])
head(res[,1:3])
res<-yt_search(term = "winston speech")
View(res)
head(res[,1:3])
View(save)
final<-head(res[,1:3])
View(final)
final<-head(res[,1:1])
View(final)
path <- "C:\\Users\\AR064679\\Documents\\thesis\\final R\\videoAnalysis\\processed\\links.txt"
final<-head(res[,1:1])
write.table(final, file = path, sep = "\t",
row.names = FALSE)
View(res)
final<-head(res[,1:1])
write.table(final, file = path, sep = "\t",
row.names = FALSE)
write.table(final, file = path, sep = "\t",
row.names = TRUE)
write.csv(my_data, file = path)
write.csv(final, file = path)
write.csv(final, file = path)
path <- "C:\\Users\\AR064679\\Documents\\thesis\\final R\\videoAnalysis\\processed\\links.csv"
write.csv(final, file = path)
path <- "C:\\Users\\AR064679\\Documents\\thesis\\final R\\videoAnalysis\\processed\\links.txt"
write.table(final, file = path, sep = "\t",
row.names = TRUE)
write.table(final, file = path, sep = '\t'
row.names = FALSE)
write.table(final, file = path, sep = "\t",
row.names = FALSE)
final<-head(res[-1,1:1])
write.table(final, file = path, sep = "\t",
row.names = FALSE)
res<-yt_search(term = "winston speech")
View(res)
final<-head(res[-1,1:1])
write.table(final, file = path, sep = "\t",
row.names = FALSE)
write.table(final, file = path)
write.table(final, file = path, sep = "\t",
row.names = FALSE)
write.table(final, file = path, sep = "\t",
row.names = FALSE,quote = FALSE)
final<- "https://www.youtube.com/watch?v=" + 'final'
final<- "https://www.youtube.com/watch?v=" + final
final <- paste("https://www.youtube.com/watch?v=", final, sep="")
view(final)
final <- paste("https://www.youtube.com/watch?v=", final, sep="")
view(final)
view(result)
library (tuber)
path <- "C:\\Users\\AR064679\\Documents\\thesis\\final R\\videoAnalysis\\processed\\links.txt"
app_id <- "151536471484-7s1bbq96drpdj03l00s0turepor9fpah.apps.googleusercontent.com"
app_secret <-"HygKXzL_WrnyLwiTfFUzsGMX"
yt_oauth(app_id, app_secret, token = "")
res<-yt_search(term = "winston speech")
View(res)
final<-head(res[-1,1:1])
result <- paste("https://www.youtube.com/watch?v=", final, sep="")
view(result)
write.table(final, file = path, sep = "\t",
row.names = FALSE,quote = FALSE)
final <- sub("^", "https://www.youtube.com/watch?v=", final )
View(final)
library (tuber)
path <- "C:\\Users\\AR064679\\Documents\\thesis\\final R\\videoAnalysis\\processed\\links.txt"
app_id <- "151536471484-7s1bbq96drpdj03l00s0turepor9fpah.apps.googleusercontent.com"
app_secret <-"HygKXzL_WrnyLwiTfFUzsGMX"
yt_oauth(app_id, app_secret, token = "")
res<-yt_search(term = "winston speech")
View(res)
final<-head(res[-1,1:1])
final <- sub("^", "https://www.youtube.com/watch?v=", final )
View(final)
write.table(final, file = path, sep = "\t",
row.names = FALSE,quote = FALSE)
library (tuber)
path <- "C:\\Users\\AR064679\\Documents\\thesis\\final R\\videoAnalysis\\processed\\links.txt"
app_id <- "151536471484-7s1bbq96drpdj03l00s0turepor9fpah.apps.googleusercontent.com"
app_secret <-"HygKXzL_WrnyLwiTfFUzsGMX"
yt_oauth(app_id, app_secret, token = "")
res<-yt_search(term = "winston speech")
View(res)
final<-head(res[-1,1:1])
write.table(final, file = path, sep = "\t",
row.names = FALSE)
final <- sub("^", "https://www.youtube.com/watch?v=", final )
library (tuber)
path <- "C:\\Users\\AR064679\\Documents\\thesis\\final R\\videoAnalysis\\processed\\links.txt"
app_id <- "151536471484-7s1bbq96drpdj03l00s0turepor9fpah.apps.googleusercontent.com"
app_secret <-"HygKXzL_WrnyLwiTfFUzsGMX"
yt_oauth(app_id, app_secret, token = "")
res<-yt_search(term = "winston speech")
View(res)
final<-head(res[-1,1:1])
View(final)
final <- sub("^", "https://www.youtube.com/watch?v=", final)
write.table(final, file = path, sep = "\t",
row.names = FALSE,quote = FALSE)
write.table(final, file = path, sep = "",
row.names = FALSE,quote = FALSE)
library(plumber)
getwd()
r <- plumb("C:/Users/AR064679/Documents/thesis/final R/senti")
r$run(port = 9000)
library(plumber)
getwd()
r <- plumb("C:/Users/AR064679/Documents/thesis/final R/senti.R")
r$run(port = 9000)
library(plumber)
getwd()
r <- plumb("C:/Users/AR064679/Documents/thesis/final R/senti.R")
r$run(port = 9000)
library(plumber)
getwd()
r <- plumb("C:/Users/AR064679/Documents/thesis/final R/senti.R")
r$run(port = 9000)
library(plumber)
getwd()
r <- plumb("C:/Users/AR064679/Documents/thesis/final R/senti.R")
r$run(port = 9000)
library(plumber)
getwd()
r <- plumb("C:/Users/AR064679/Documents/thesis/final R/senti.R")
r$run(port = 9000)
library(plumber)
getwd()
r <- plumb("C:/Users/AR064679/Documents/thesis/final R/senti.R")
r$run(port = 9000)
library (tuber)
path <- "C:\\Users\\AR064679\\Documents\\thesis\\final R\\videoAnalysis\\processed\\links.txt"
app_id <- "151536471484-d9lcn5ms69arj6f10v3ja4opdf718ppq.apps.googleusercontent.com"
app_secret <-"AmPrW7gfBKqJdZ8_LpUXKNVJ"
yt_oauth(app_id, app_secret, token = "")
res<-yt_search(term = "winston speech")
View(res)
final<-head(res[-1,1:1])
View(final)
final <- sub("^", "https://www.youtube.com/watch?v=", final)
write.table(final, file = path, sep = "",
row.names = FALSE,quote = FALSE)
